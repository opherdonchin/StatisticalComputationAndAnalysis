# -*- coding: utf-8 -*-
"""Homework2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ypokJFgW_ZY1H1C4cSS8IYm4t1bnCjuy

# Homework 2

Please put the statement of the problem here. You can copy / paste or summarize however you like.

## Discussion of results

Once you are done with the homework, please add a section at the top where you say what you actually found.

## Setup

This section is for imports and such
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import arviz as az
import matplotlib.pyplot as plt
import pymc as pm
import numpy as np
import pandas as pd
from scipy import stats
import seaborn as sns
from google.colab import files

!pip install preliz
import preliz as pz

az.style.use("arviz-darkgrid")
sns.set_style("darkgrid")

"""## 1. Question 1"""

#create data
trials = 4
theta_real = 0.35 # unknown value in a real experiment
data = pz.Binomial(n=1, p=theta_real).rvs(trials)
data

#creating our three models
coords = {"data": np.arange(len(data))}

with pm.Model(coords=coords) as model_1:
    thet = pm.Beta('thet', alpha=1., beta=1.)
    y = pm.Bernoulli('y', p=thet, observed=data, dims = 'data')
    idata1 = pm.sample(1000, chains = 4)


with pm.Model(coords=coords) as model_2:
    thet = pm.Uniform('thet', lower=0, upper=1)
    y = pm.Bernoulli('y', p=thet, observed=data, dims = 'data')
    idata2 = pm.sample(1000, chains = 4)


with pm.Model(coords=coords) as model_3:
    thet = pm.Uniform('thet', lower=-1, upper=2)
    y = pm.Bernoulli('y', p=thet, observed=data, dims = 'data')
    idata3 = pm.sample(1000, chains = 4)

#b. creating the model graphics using model_to_graphviz
pm.model_to_graphviz(model_1)
pm.model_to_graphviz(model_2)
pm.model_to_graphviz(model_3)

#d. trace plots
az.plot_trace(idata1, compact = False)

plt.savefig("trace1.png", bbox_inches='tight')
files.download("trace1.png")

az.plot_trace(idata2, compact = False)

plt.savefig("trace2.png", bbox_inches='tight')
files.download("trace2.png")

#d. HDI
az.plot_posterior(idata1)

plt.savefig("hdi1.png", bbox_inches='tight')
files.download("hdi1.png")

az.plot_posterior(idata2)

plt.savefig("hdi2.png", bbox_inches='tight')
files.download("hdi2.png")

"""## 2. Question 2"""

#loading data
data = np.loadtxt("https://github.com/aloctavodia/BAP3/raw/refs/heads/main/code/data/chemical_shifts.csv")
data

#c. setting up priors
mu_prior = np.mean(data)
s = np.std(data, ddof = 1)

sig_prior1 = s
sig_prior2 = 3*s
sig_prior3 = 10*s

#model
coords = {"data": np.arange(len(data))}

with pm.Model(coords=coords) as model_1:
  m = pm.Normal('m', mu = mu_prior, sigma = sig_prior1)
  sig = pm.HalfNormal('sig', sigma = 5)
  Y = pm.Normal('Y', mu=m, sigma=sig, observed=data, dims = 'data')
  idata_1 = pm.sample(1000, chains = 4)


with pm.Model(coords=coords) as model_2:
  m = pm.Normal('m', mu = mu_prior, sigma = sig_prior2)
  sig = pm.HalfNormal('sig', sigma = 5)
  Y = pm.Normal('Y', mu=m, sigma=sig, observed=data, dims = 'data')
  idata_2 = pm.sample(1000, chains = 4)


with pm.Model(coords=coords) as model_3:
  m = pm.Normal('m', mu = mu_prior, sigma = sig_prior3)
  sig = pm.HalfNormal('sig', sigma = 5)
  Y = pm.Normal('Y', mu=m, sigma=sig, observed=data, dims = 'data')
  idata_3 = pm.sample(1000, chains = 4)

#trace plots
az.plot_trace(idata_1, compact = False)
plt.subplots_adjust(hspace=0.6)  # Increase spacing between subplots

plt.savefig("trace1.png", bbox_inches='tight')
files.download("trace1.png")

az.plot_trace(idata_2, compact = False)
plt.subplots_adjust(hspace=0.6)  # Increase spacing between subplots

plt.savefig("trace2.png", bbox_inches='tight')
files.download("trace2.png")

az.plot_trace(idata_3, compact = False)
plt.subplots_adjust(hspace=0.6)  # Increase spacing between subplots

plt.savefig("trace3.png", bbox_inches='tight')
files.download("trace3.png")

#hdi
az.plot_posterior(idata_1)

az.plot_posterior(idata_2)

az.plot_posterior(idata_3)

#posterior predictive checks
pm.sample_posterior_predictive(idata_1, model=model_1, extend_inferencedata=True)
pm.sample_posterior_predictive(idata_2, model=model_2, extend_inferencedata=True)
pm.sample_posterior_predictive(idata_3, model=model_3, extend_inferencedata=True)


az.plot_ppc(idata_1, num_pp_samples=100)
az.plot_ppc(idata_2, num_pp_samples=100)
az.plot_ppc(idata_3, num_pp_samples=100)

"""## 3. Question 3"""

#model
with pm.Model(coords=coords) as model_t:
  m = pm.Normal('m', mu = mu_prior, sigma = sig_prior2)
  sig = pm.HalfNormal('sig', sigma = 5)
  n = pm.Exponential('n', 1/10)
  Y = pm.StudentT('Y', nu = n, mu=m, sigma=sig, observed=data, dims = 'data')
  idata_t = pm.sample(1000, chains = 4)

#trace data
az.plot_trace(idata_t, compact = False)
plt.subplots_adjust(hspace=0.6)  # Increase spacing between subplots

#plotting the posterior with the HDI
az.plot_posterior(idata_t)

#Posterior predictive check
pm.sample_posterior_predictive(idata_t, model=model_t, extend_inferencedata=True)

az.plot_ppc(idata_t,  num_pp_samples=100)

"""## 4. Question 4"""

#a.
#mean and standard deviation with outliers

m_all = np.mean(data)
s_all = np.std(data, ddof = 1)

#computing IQR
#plt.boxplot(data, vert=False)

iqr = np.subtract(*np.percentile(data, [75, 25]))

data_no_outliers = data[(data >= np.percentile(data, 25) - 1.5 * iqr) & (data <= np.percentile(data, 75) + 1.5 * iqr)]

m_no_outliers = np.mean(data_no_outliers)
s_no_outliers = np.std(data_no_outliers, ddof = 1)


print(f'Removing outliers removed {len(data)-len(data_no_outliers)} data points')
print(f'The mean and standard devation of all the data is {round(m_all, 2), round(s_all, 2)}')
print(f'The mean and standard devation of the data without outliers is {round(m_no_outliers, 2), round(s_no_outliers, 2)}')

#c. adding more outliers
np.percentile(data, 25) - 1.5 * iqr
np.percentile(data, 25) + 1.5 * iqr

data_more_outliers = np.concatenate((data, np.array([40, 60])))

plt.boxplot(data_more_outliers, vert=False)

#normal model
with pm.Model() as model_2:
  m = pm.Normal('m', mu = mu_prior, sigma = sig_prior2)
  sig = pm.HalfNormal('sig', sigma = 5)
  Y = pm.Normal('Y', mu=m, sigma=sig, observed=data_more_outliers)
  idata_2 = pm.sample(1000, chains = 4)

az.plot_trace(idata_2, compact = False)
az.plot_posterior(idata_2)
pm.sample_posterior_predictive(idata_2, model=model_2, extend_inferencedata=True)
az.plot_ppc(idata_2, num_pp_samples=100)

#t-model
with pm.Model() as model_t:
  m = pm.Normal('m', mu = mu_prior, sigma = sig_prior2)
  sig = pm.HalfNormal('sig', sigma = 5)
  n = pm.Exponential('n', 1/10)
  Y = pm.StudentT('Y', nu = n, mu=m, sigma=sig, observed=data_more_outliers)
  idata_t = pm.sample(1000, chains = 4)

az.plot_trace(idata_t, compact = False)
az.plot_posterior(idata_t)
pm.sample_posterior_predictive(idata_t, model=model_t, extend_inferencedata=True)
az.plot_ppc(idata_t,  num_pp_samples=100)

"""##5. Question 5"""

tips = pd.read_csv("https://github.com/aloctavodia/BAP3/raw/refs/heads/main/code/data//tips.csv")

categories = np.array(["Thur", "Fri", "Sat", "Sun"])
bill = tips["total_bill"].values
idx = pd.Categorical(tips["day"], categories=categories).codes

#creating the model
coords = {"days": categories, "days_flat":categories[idx]}

with pm.Model(coords=coords) as comparing_groups:
  m = pm.HalfNormal("m", sigma = 20, dims="days")
  sig = pm.HalfNormal("sig", sigma=30, dims="days")

  y = pm.Gamma("y", mu=m[idx], sigma=sig[idx], observed = bill, dims = "days_flat")

  idata_cg = pm.sample(1000, chains = 4)
  idata_cg.extend(pm.sample_posterior_predictive(idata_cg))

#looking at the inference data object
idata_cg

#b. 2. looking at the posterior for the mean on Sundays
mean_sun = idata_cg.sel(days="Sun") #all measurements in 2013
mean_sun

#b. 3. distributions of mean differences between Thursday and Sunday
cg_posterior = az.extract(idata_cg)
means_diff = cg_posterior["m"].sel(days='Thur') - cg_posterior['m'].sel(days='Sun')

az.plot_posterior(means_diff.values, ref_val=0)

means_diff

"""# Watermark

Please include this watermark so I can see what versions you are working with
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install watermark

# Commented out IPython magic to ensure Python compatibility.
# %load_ext watermark
# %watermark -v -iv -w